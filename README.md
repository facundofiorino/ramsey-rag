# Frank Ramsey RAG System

A complete RAG (Retrieval-Augmented Generation) system for querying Frank Ramsey's philosophical works using semantic search and local LLMs.

## ğŸ¯ Quick Start

### Ask Questions About Ramsey's Philosophy

```bash
# Interactive mode (recommended)
python ask_ramsey.py -i

# Single question
python ask_ramsey.py "What did Ramsey think about truth?"

# Use a better model for complex questions
python ask_ramsey.py -i --model qwen2.5:14b
```

### Example Questions

- "What did Ramsey think about truth?"
- "How did Ramsey approach probability?"
- "What is Ramsey's view on causality?"
- "What did Ramsey say about the foundations of mathematics?"
- "How did Ramsey's pragmatism differ from Peirce and James?"

## ğŸ“Š What You Have

**Corpus**: 583,862 words from 8 philosophical texts
**Vector Store**: 4,696 searchable chunks with semantic embeddings
**Quality**: 85-90% average (high-quality PDFs + OCR-corrected scans)
**Cost**: FREE (100% local, no API costs)

## ğŸ“ Project Structure

```
ramsey_training/
â”œâ”€â”€ ask_ramsey.py              # ğŸ¯ MAIN SCRIPT - Full RAG Q&A system
â”‚
â”œâ”€â”€ ramsey_data/               # Source PDFs (8 books, 58MB)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ extracted/             # Extracted text files
â”‚   â”œâ”€â”€ training/              # Training corpus
â”‚   â”‚   â”œâ”€â”€ ramsey_corpus_full.txt      (584K words)
â”‚   â”‚   â”œâ”€â”€ ramsey_corpus_chunks.jsonl  (pre-chunked)
â”‚   â”‚   â””â”€â”€ corpus_metadata.json        (statistics)
â”‚   â””â”€â”€ vectorstore/           # Semantic search database (50MB)
â”‚
â”œâ”€â”€ src/                       # Extraction pipeline code
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ extract_all.py     # PDF extraction orchestrator
â”‚       â”œâ”€â”€ processors/        # OCR, spell-check, LLM correction
â”‚       â””â”€â”€ extractors/        # PDF text extraction
â”‚
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ prepare_complete_corpus.py  # Build training corpus
â”‚   â”œâ”€â”€ create_rag_system.py        # Create vector database
â”‚   â”œâ”€â”€ demo_ramsey_rag.py          # Semantic search demo
â”‚   â”œâ”€â”€ run_llm_correction_pipeline.sh
â”‚   â”œâ”€â”€ utilities/             # Helper scripts
â”‚   â””â”€â”€ archive/               # Old one-off scripts
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ FULL_RAG_COMPLETE.md   # Complete guide & examples
â”‚   â”œâ”€â”€ README_RAG.md          # Quick start guide
â”‚   â”œâ”€â”€ TRAINING_GUIDE.md      # Advanced training options
â”‚   â””â”€â”€ guides/                # OCR & extraction guides
â”‚
â”œâ”€â”€ logs/                      # Processing logs
â””â”€â”€ venv/                      # Python virtual environment
```

## ğŸš€ Installation

### Prerequisites

```bash
# Python 3.8+
python --version

# Ollama (for LLM)
ollama --version
```

### Setup

```bash
# Activate virtual environment
source venv/bin/activate

# Install RAG dependencies (if needed)
pip install langchain-ollama langchain-chroma langchain-community sentence-transformers

# Pull Ollama models (if needed)
ollama pull llama3:latest
ollama pull qwen2.5:14b  # Optional, for better quality
```

### Verify Installation

```bash
# Check vector store exists
ls data/vectorstore/

# Test query
python ask_ramsey.py "What did Ramsey think about truth?"
```

## ğŸ“š How It Works

### 4-Phase Pipeline

**Phase 1: Document Extraction**
```
PDFs â†’ Text Extraction â†’ OCR (if needed) â†’ Spell Check â†’ LLM Correction â†’ Clean Text
```

**Phase 2: Corpus Preparation**
```
8 Text Files â†’ Consolidation â†’ 584K word corpus â†’ Pre-chunked for training
```

**Phase 3: RAG Setup**
```
Corpus â†’ Text Splitting â†’ Vector Embeddings â†’ ChromaDB â†’ Searchable Index
```

**Phase 4: Question Answering**
```
Question â†’ Semantic Search â†’ Top Passages â†’ Ollama LLM â†’ Answer + Citations
```

## ğŸ’¡ Command-Line Options

```bash
# Interactive mode
python ask_ramsey.py -i

# Single question
python ask_ramsey.py "your question here"

# Use different model (slower but better quality)
python ask_ramsey.py -i --model qwen2.5:14b

# More source passages for complex questions
python ask_ramsey.py "complex question" --sources 6

# Quiet mode (answer only)
python ask_ramsey.py "question" -q
```

## ğŸ¤– Available Models

You have these Ollama models installed:
- **llama3:latest** (default) - Fast, good quality
- **qwen2.5:latest** - Good balance
- **qwen2.5:14b** - Best quality (slower, 9GB)
- **mistral:latest** - Alternative
- **deepseek-r1:latest** - Reasoning-focused

Recommended: Start with **llama3:latest**, use **qwen2.5:14b** for complex philosophical questions.

## ğŸ“– Source Materials (8 Books)

1. **Frank Ramsey: a sheer excess of powers** - 225,555 words
2. **Frank Ramsey and the Realistic Spirit** - 119,687 words
3. **Ramsey's legacy** - 88,564 words
4. **On Truth: Original Manuscripts** - 63,793 words
5. **Truth and Success** (OCR) - 44,765 words
6. **Shooting Star Biography** - 25,112 words
7. **General Propositions** (OCR) - 8,663 words
8. **Theories** - 7,558 words

**Total**: 583,862 words | 4,696 searchable chunks | All 8 PDFs processed âœ“

## ğŸ› ï¸ Troubleshooting

**"Vector store not found"**
```bash
python scripts/demo_ramsey_rag.py  # Creates it automatically
```

**"Model not found"**
```bash
ollama list              # See available models
ollama pull llama3       # Download if needed
```

**Slow responses**
- First query loads models (slow)
- Subsequent queries are faster
- Try smaller model: `--model qwen2.5:latest`

**Poor quality answers**
- Use more sources: `--sources 6`
- Try better model: `--model qwen2.5:14b`
- Make question more specific

## ğŸ“ Documentation

- **docs/FULL_RAG_COMPLETE.md** - Complete usage guide with examples
- **docs/README_RAG.md** - Quick start guide
- **docs/TRAINING_GUIDE.md** - Advanced training options (fine-tuning, etc.)
- **docs/guides/** - OCR extraction pipeline documentation

## ğŸ’ª Tips for Better Answers

1. **Be Specific**: "What did Ramsey think about truth?" vs "Tell me about Ramsey"

2. **Use More Sources for Complex Questions**:
   ```bash
   python ask_ramsey.py "Explain Ramsey's theory of universals" --sources 6
   ```

3. **Try Different Models**:
   - llama3:latest - Fast, conversational
   - qwen2.5:14b - Slower, more thorough

4. **Check Sources**: Always review source passages to verify AI's interpretation

## âš¡ Performance

- **Search Speed**: <1 second
- **Answer Generation**: 5-30 seconds (model dependent)
- **Accuracy**: High (grounded in actual Ramsey texts)
- **Cost**: FREE (100% local)

## ğŸŒŸ What Makes This Special

- **Grounded**: Answers based on ACTUAL Ramsey texts, not hallucinations
- **Cited**: Every answer shows source passages
- **Free**: No API costs, runs entirely locally
- **Fast**: Semantic search across 584K words in <1 second
- **Quality**: 85-90% text quality from advanced OCR + LLM correction

## ğŸ“œ License

For educational and research purposes. Source texts are in the public domain or used under fair use.

## ğŸ™ Credits

Built on:
- **584,000 words** from 8 philosophical texts
- **OCR**: Tesseract at 800 DPI
- **LLM Correction**: qwen2.5:14b
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2
- **Vector DB**: ChromaDB
- **LLM**: Ollama (local)

---

**Questions?** See `docs/FULL_RAG_COMPLETE.md` for detailed examples and advanced usage.
